{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(current_dir.replace('\\code', '\\data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_ii = pd.read_csv('best_ball_mania_ii.csv')\n",
    "data_iii = pd.read_csv('best_ball_mania_iii.csv')\n",
    "data_iv = pd.read_csv('best_ball_mania_iv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase 'roster_points' and 'pick_points' for dataset iv by 4.5\n",
    "data_iv['pick_points'] = data_iv['pick_points']*4.5\n",
    "data_iv['roster_points'] = data_iv['roster_points']*4.5\n",
    "\n",
    "# Columns that are in all datasets\n",
    "cols = list(set(data_ii.columns) & set(data_iii.columns) & set(data_iv.columns))\n",
    "\n",
    "# Filter columns\n",
    "data_ii = data_ii[cols]\n",
    "data_iii = data_iii[cols]\n",
    "data_iv = data_iv[cols]\n",
    "\n",
    "# Concatenate data\n",
    "data = pd.concat([data_ii, data_iii, data_iv], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'tournament_entry_id' to 'team_id'\n",
    "data = data.rename(columns={'tournament_entry_id': 'team_id'})\n",
    "\n",
    "data = data.drop(columns=['clock', 'tournament_round_number', 'bye_week', 'draft_time'])\n",
    "\n",
    "# Fit a polynomial regression\n",
    "p = np.poly1d(np.polyfit(data['overall_pick_number'], data['pick_points'], 2))\n",
    "data['poly_points'] = p(data['overall_pick_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap the points per position rank\n",
    "# Find distributions of points per position rank for each position\n",
    "# Simulate a single draft with 30 QBs, 70 RBs, 100 WRs, and 30 TEs\n",
    "# Each pick is a random number from the distribution of *centered* points per position rank\n",
    "# Find the chance that RB1 scores more points than WR1 and so on\n",
    "\n",
    "def polynomial_points(position_name):\n",
    "    # Group by 'draft_id', 'position_name', 'pick_points' and 'overall_pick_number'\n",
    "    data_grouped = data.groupby(['draft_id', 'position_name', 'overall_pick_number', 'pick_points']).mean().reset_index()\n",
    "    data_grouped = data_grouped[['draft_id', 'position_name', 'overall_pick_number', 'pick_points']]\n",
    "    data_grouped = data_grouped.drop(data_grouped[data_grouped['position_name'] == 'FB'].index)\n",
    "\n",
    "    # New column 'pos_rank' which is the rank of the 'position_name' in each 'draft_id'\n",
    "    # if lowest 'overall_pick_number' then rank is 1, if second lowest then rank is 2, and so on\n",
    "    data_grouped['pos_rank'] = data_grouped.groupby(['draft_id', 'position_name'])['overall_pick_number'].rank()\n",
    "\n",
    "    # Limit by these parameters: 30 QBs, 70 RBs, 100 WRs, and 30 TEs\n",
    "    data_grouped = data_grouped[(data_grouped['position_name'] == 'QB') & (data_grouped['pos_rank'] <= 30) |\n",
    "                                (data_grouped['position_name'] == 'RB') & (data_grouped['pos_rank'] <= 70) |\n",
    "                                (data_grouped['position_name'] == 'WR') & (data_grouped['pos_rank'] <= 100) |\n",
    "                                (data_grouped['position_name'] == 'TE') & (data_grouped['pos_rank'] <= 30)]\n",
    "\n",
    "    # Fit a polynomial regression to the data\n",
    "    p = np.poly1d(np.polyfit(data_grouped[data_grouped['position_name'] == position_name]['pos_rank'],\n",
    "                              data_grouped[data_grouped['position_name'] == position_name]['pick_points'], 2))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def variance_regression(position_name):\n",
    "    # Group by 'draft_id', 'position_name', 'pick_points' and 'overall_pick_number'\n",
    "    data_grouped = data.groupby(['draft_id', 'position_name', 'overall_pick_number', 'pick_points']).mean().reset_index()\n",
    "    data_grouped = data_grouped[['draft_id', 'position_name', 'overall_pick_number', 'pick_points']]\n",
    "    data_grouped = data_grouped.drop(data_grouped[data_grouped['position_name'] == 'FB'].index)\n",
    "\n",
    "    # New column 'pos_rank' which is the rank of the 'position_name' in each 'draft_id'\n",
    "    # if lowest 'overall_pick_number' then rank is 1, if second lowest then rank is 2, and so on\n",
    "    data_grouped['pos_rank'] = data_grouped.groupby(['draft_id', 'position_name'])['overall_pick_number'].rank()\n",
    "\n",
    "    # Limit by these parameters: 30 QBs, 70 RBs, 100 WRs, and 30 TEs\n",
    "    data_grouped = data_grouped[(data_grouped['position_name'] == 'QB') & (data_grouped['pos_rank'] <= 30) |\n",
    "                                (data_grouped['position_name'] == 'RB') & (data_grouped['pos_rank'] <= 70) |\n",
    "                                (data_grouped['position_name'] == 'WR') & (data_grouped['pos_rank'] <= 100) |\n",
    "                                (data_grouped['position_name'] == 'TE') & (data_grouped['pos_rank'] <= 30)]\n",
    "    \n",
    "    # Fit a linear regression to the standard deviation of the points per position rank\n",
    "    grouped_data = data_grouped[data_grouped['position_name'] == position_name].groupby('pos_rank')['pick_points'].std()\n",
    "    p = np.poly1d(np.polyfit(grouped_data.index, grouped_data.values, 1))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that estimates the points for each position rank\n",
    "def estimations(position_name, max, starter):\n",
    "    points = polynomial_points(position_name)\n",
    "    variance = variance_regression(position_name)\n",
    "    df = pd.DataFrame()\n",
    "    df['rank'] = np.arange(1, max+1)\n",
    "    df['pos_rank'] = position_name + df['rank'].astype(str)\n",
    "    df['points'] = points(df['rank'])\n",
    "    df['top_five'] = points(df['rank']) + 2*variance(df['rank'])\n",
    "    df['bottom_five'] = points(df['rank']) - 2*variance(df['rank'])\n",
    "    mu = df[df['rank'] == starter]['points'].values[0]\n",
    "    df['points'] -= mu\n",
    "    df['top_five'] -= mu\n",
    "    df['bottom_five'] -= mu\n",
    "    return df\n",
    "\n",
    "# Estimations for each position\n",
    "qb = estimations('QB', max=30, starter=12)\n",
    "rb = estimations('RB', max=70, starter=28)\n",
    "wr = estimations('WR', max=100, starter=40)\n",
    "te = estimations('TE', max=30, starter=16)\n",
    "\n",
    "# New df with all positions\n",
    "df_points = pd.concat([qb[['pos_rank', 'points', 'top_five', 'bottom_five']], \n",
    "                rb[['pos_rank', 'points', 'top_five', 'bottom_five']], \n",
    "                wr[['pos_rank', 'points', 'top_five', 'bottom_five']], \n",
    "                te[['pos_rank', 'points', 'top_five', 'bottom_five']]], axis=0)\n",
    "df_points = df_points.sort_values(by='points', ascending=False)\n",
    "df_points.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QB1.0</th>\n",
       "      <th>QB2.0</th>\n",
       "      <th>QB3.0</th>\n",
       "      <th>RB1.0</th>\n",
       "      <th>RB2.0</th>\n",
       "      <th>RB3.0</th>\n",
       "      <th>RB4.0</th>\n",
       "      <th>RB5.0</th>\n",
       "      <th>RB6.0</th>\n",
       "      <th>RB7.0</th>\n",
       "      <th>...</th>\n",
       "      <th>WR1.0</th>\n",
       "      <th>WR2.0</th>\n",
       "      <th>WR3.0</th>\n",
       "      <th>WR4.0</th>\n",
       "      <th>WR5.0</th>\n",
       "      <th>WR6.0</th>\n",
       "      <th>WR7.0</th>\n",
       "      <th>WR8.0</th>\n",
       "      <th>WR9.0</th>\n",
       "      <th>roster_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1622.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>1177.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1487.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1633.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1752.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>34.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1390.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>80.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>1685.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>37.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1629.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>60.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1788.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>101.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1395.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       QB1.0  QB2.0  QB3.0  RB1.0  RB2.0  RB3.0  RB4.0  RB5.0  RB6.0  RB7.0  \\\n",
       "0       36.0  156.0  432.0   13.0  108.0  109.0  157.0  180.0  205.0  432.0   \n",
       "1       47.0  194.0  432.0   26.0   74.0   95.0   98.0  146.0  432.0  432.0   \n",
       "2       68.0  188.0  432.0    5.0   20.0  125.0  140.0  149.0  432.0  432.0   \n",
       "3      115.0  126.0  432.0    6.0   43.0  139.0  150.0  198.0  432.0  432.0   \n",
       "4       31.0  151.0  432.0    7.0   55.0   90.0  114.0  138.0  175.0  199.0   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "35995   34.0  106.0  183.0   10.0   39.0   82.0  159.0  178.0  207.0  432.0   \n",
       "35996   80.0  104.0  432.0    8.0   17.0  152.0  176.0  432.0  432.0  432.0   \n",
       "35997   37.0  132.0  432.0   36.0   60.0  109.0  181.0  205.0  432.0  432.0   \n",
       "35998   60.0  156.0  204.0   13.0   36.0   84.0   85.0  205.0  432.0  432.0   \n",
       "35999  101.0  116.0  197.0    5.0   20.0   29.0  149.0  188.0  212.0  432.0   \n",
       "\n",
       "       ...  WR1.0  WR2.0  WR3.0  WR4.0  WR5.0  WR6.0  WR7.0  WR8.0  WR9.0  \\\n",
       "0      ...   12.0   60.0   61.0   84.0   85.0  133.0  181.0  204.0  432.0   \n",
       "1      ...    2.0   23.0   50.0  119.0  122.0  143.0  167.0  191.0  215.0   \n",
       "2      ...   29.0   44.0   53.0   77.0   92.0  116.0  164.0  173.0  212.0   \n",
       "3      ...   19.0   30.0   67.0   78.0   91.0  102.0  163.0  187.0  211.0   \n",
       "4      ...   42.0   66.0   79.0  103.0  127.0  186.0  210.0  432.0  432.0   \n",
       "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "35995  ...   15.0   58.0   63.0   87.0  111.0  135.0  154.0  432.0  432.0   \n",
       "35996  ...   32.0   41.0   56.0   65.0   89.0  113.0  137.0  200.0  209.0   \n",
       "35997  ...   12.0   13.0   84.0   85.0  108.0  133.0  157.0  180.0  432.0   \n",
       "35998  ...   12.0   37.0   61.0  108.0  109.0  132.0  133.0  180.0  432.0   \n",
       "35999  ...   53.0   68.0   77.0   92.0  125.0  140.0  432.0  432.0  432.0   \n",
       "\n",
       "       roster_points  \n",
       "0            1622.50  \n",
       "1            1177.38  \n",
       "2            1487.10  \n",
       "3            1633.24  \n",
       "4            1752.82  \n",
       "...              ...  \n",
       "35995        1390.50  \n",
       "35996        1685.38  \n",
       "35997        1629.99  \n",
       "35998        1788.08  \n",
       "35999        1395.09  \n",
       "\n",
       "[36000 rows x 23 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ordinal_ranks(data, drop_columns=False):\n",
    "    # Group by 'draft_id', 'position_name', 'pick_points' and 'overall_pick_number'\n",
    "    df = data[['team_id', 'position_name', 'overall_pick_number', 'pick_points']]\n",
    "    roster_points = data[['team_id', 'roster_points']].drop_duplicates()\n",
    "\n",
    "    data_grouped = df.groupby(['team_id', 'position_name', 'overall_pick_number', 'pick_points']).mean().reset_index()\n",
    "    data_grouped = data_grouped[['team_id', 'position_name', 'overall_pick_number', 'pick_points']]\n",
    "    data_grouped = data_grouped.drop(data_grouped[data_grouped['position_name'] == 'FB'].index)\n",
    "\n",
    "    # New column 'pos_rank' which is the rank of the 'position_name' in each 'draft_id'\n",
    "    # if lowest 'overall_pick_number' then rank is 1, if second lowest then rank is 2, and so on\n",
    "    data_grouped['pos_rank'] = data_grouped.groupby(['team_id', 'position_name'])['overall_pick_number'].rank()\n",
    "\n",
    "    # New column pos_team_rank which is position_name + pos_rank\n",
    "    data_grouped['pos_team_rank'] = data_grouped['position_name'] + data_grouped['pos_rank'].astype(str)\n",
    "\n",
    "    # Pivot the data so that position names are columns, and overall pick numbers are rows\n",
    "    data_grouped = data_grouped.pivot(index='team_id', columns='pos_team_rank', values='overall_pick_number').reset_index()\n",
    "\n",
    "    # Drop columns below 5 percent full\n",
    "    if drop_columns == True:\n",
    "        data_grouped = data_grouped.dropna(thresh=0.05*len(data_grouped), axis=1)\n",
    "\n",
    "    # Fill NaN values with 432\n",
    "    data_grouped = data_grouped.fillna(432)\n",
    "\n",
    "    # Merge with 'roster_points'\n",
    "    data_grouped = data_grouped.merge(roster_points, on='team_id')\n",
    "    \n",
    "    # Drop 'team_id' column\n",
    "    data_grouped = data_grouped.drop(columns=['team_id'])\n",
    "\n",
    "    return data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_got_encode(data):\n",
    "    df = data[['team_pick_number', 'position_name', 'roster_points', 'team_id']]\n",
    "\n",
    "    roster_points = data[['team_id', 'roster_points']].drop_duplicates()\n",
    "\n",
    "    # Group by 'team_id' and 'team_pick_number' and aggregate 'position_name'\n",
    "    df = df.groupby(['team_id', 'team_pick_number'])['position_name'].first().reset_index()\n",
    "\n",
    "    # Now pivot the data\n",
    "    df = df.pivot(index='team_id', columns='team_pick_number', values='position_name').reset_index()\n",
    "\n",
    "    # Add 'roster_points' to the data without adding new rows\n",
    "    df = pd.merge(df, roster_points, on='team_id')\n",
    "    df = df.drop(columns='team_id')\n",
    "    df = df.sample(frac=1.0, random_state=0)\n",
    "\n",
    "    # One-hot encode the 'position_name' column\n",
    "    columns_to_encode = list(range(1, 19))\n",
    "    df = pd.get_dummies(df, columns=columns_to_encode)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_indicies(data):\n",
    "    # Converting positions to numerical indicies for the model\n",
    "    positions = ['QB', 'RB', 'WR', 'TE', 'FB']\n",
    "    position_dict = {position: i for i, position in enumerate(positions)}\n",
    "\n",
    "    # Load data\n",
    "    df = data[['team_pick_number', 'position_name', 'roster_points', 'team_id']]\n",
    "    roster_points = data[['team_id', 'roster_points']].drop_duplicates()\n",
    "\n",
    "    # Group by 'team_id' and 'team_pick_number' and aggregate 'position_name'\n",
    "    df = df.groupby(['team_id', 'team_pick_number'])['position_name'].first().reset_index()\n",
    "\n",
    "    # Now pivot the data\n",
    "    df = df.pivot(index='team_id', columns='team_pick_number', values='position_name').reset_index()\n",
    "\n",
    "    # Add 'roster_points' to the data without adding new rows\n",
    "    df = pd.merge(df, roster_points, on='team_id')\n",
    "    df = df.drop(columns='team_id')\n",
    "\n",
    "    # Use the position_dict to convert the 'position_name' to a numerical index\n",
    "    df = df.map(lambda x: position_dict[x] if x in position_dict else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04464058977276375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load data\n",
    "#   df = one_got_encode(data)\n",
    "#   df = numerical_indicies(data)\n",
    "df = ordinal_ranks(data, drop_columns=True)\n",
    "\n",
    "# Split the df into training and testing sets\n",
    "X = df.drop(columns='roster_points')\n",
    "y = df['roster_points']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Fit a random forest regressor\n",
    "rfr = RandomForestRegressor(n_estimators=500, \n",
    "                            max_depth=15,\n",
    "                            min_samples_leaf=15,\n",
    "                            min_samples_split=8)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Predict roster_points\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "# Calculate the R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "{'max_depth': 20, 'min_samples_leaf': 20, 'min_samples_split': 15, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [5, 15],\n",
    "    'min_samples_leaf': [10, 20],\n",
    "}\n",
    "\n",
    "# Load data\n",
    "#   df = one_got_encode(data)\n",
    "#   df = numerical_indicies(data)\n",
    "df = ordinal_ranks(data, drop_columns=True)\n",
    "\n",
    "# Split the df into training and testing sets\n",
    "X = df.drop(columns='roster_points')\n",
    "y = df['roster_points']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Fit a random forest regressor\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 602973.8750 - val_loss: 29124.2910\n",
      "Epoch 2/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 25597.4766 - val_loss: 27841.9258\n",
      "Epoch 3/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 26172.7871 - val_loss: 27476.2090\n",
      "Epoch 4/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25844.1074 - val_loss: 26827.5859\n",
      "Epoch 5/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25221.6953 - val_loss: 25709.6797\n",
      "Epoch 6/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25520.9688 - val_loss: 26800.1113\n",
      "Epoch 7/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24952.9570 - val_loss: 26463.8145\n",
      "Epoch 8/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 24989.3301 - val_loss: 26817.2793\n",
      "Epoch 9/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25182.4824 - val_loss: 25361.0273\n",
      "Epoch 10/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25187.8828 - val_loss: 25561.6152\n",
      "Epoch 11/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25065.6816 - val_loss: 24927.9609\n",
      "Epoch 12/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23888.6367 - val_loss: 25059.0977\n",
      "Epoch 13/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24505.2031 - val_loss: 24521.3203\n",
      "Epoch 14/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24226.4180 - val_loss: 25823.1309\n",
      "Epoch 15/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24565.4609 - val_loss: 25049.6309\n",
      "Epoch 16/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24293.6016 - val_loss: 25203.4160\n",
      "Epoch 17/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24317.5898 - val_loss: 25578.8398\n",
      "Epoch 18/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24120.4766 - val_loss: 24350.6426\n",
      "Epoch 19/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24406.3613 - val_loss: 24214.9492\n",
      "Epoch 20/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23666.8926 - val_loss: 24976.3477\n",
      "Epoch 21/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23836.0488 - val_loss: 24821.1582\n",
      "Epoch 22/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23732.7109 - val_loss: 24078.8066\n",
      "Epoch 23/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23561.2578 - val_loss: 24387.8750\n",
      "Epoch 24/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23628.4824 - val_loss: 23826.6094\n",
      "Epoch 25/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23687.9531 - val_loss: 24330.2129\n",
      "Epoch 26/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23591.6973 - val_loss: 23833.8926\n",
      "Epoch 27/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23495.8340 - val_loss: 23737.1328\n",
      "Epoch 28/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23613.3633 - val_loss: 24045.6328\n",
      "Epoch 29/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23303.5078 - val_loss: 24009.4258\n",
      "Epoch 30/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23757.2129 - val_loss: 23621.9141\n",
      "Epoch 31/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23391.4980 - val_loss: 23880.9062\n",
      "Epoch 32/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23540.9746 - val_loss: 25671.2090\n",
      "Epoch 33/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23523.4102 - val_loss: 23686.9297\n",
      "Epoch 34/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23305.7305 - val_loss: 27068.2012\n",
      "Epoch 35/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23678.7090 - val_loss: 23900.0645\n",
      "Epoch 36/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23790.1797 - val_loss: 23942.2051\n",
      "Epoch 37/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23382.5195 - val_loss: 23665.9863\n",
      "Epoch 38/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23578.9102 - val_loss: 24356.5371\n",
      "Epoch 39/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23206.3105 - val_loss: 24199.5273\n",
      "Epoch 40/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 23011.1875 - val_loss: 25645.3770\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step\n",
      "-0.11677123450667115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load data\n",
    "#   df = numerical_indicies(data)\n",
    "df = ordinal_ranks(data, drop_columns=False)\n",
    "\n",
    "# Split the df into training and testing sets\n",
    "X = df.drop(columns='roster_points')\n",
    "y = df['roster_points']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(0.01), loss='mse')\n",
    "model.fit(x=X_train, y=y_train.values, \n",
    "          validation_data=(X_test, y_test.values), \n",
    "          batch_size=50, epochs=40)\n",
    "# Predict roster_points\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 1976162.1250 - val_loss: 1112360.2500\n",
      "Epoch 2/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 917505.1250 - val_loss: 453926.6875\n",
      "Epoch 3/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 359387.9375 - val_loss: 155719.6875\n",
      "Epoch 4/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 121903.8516 - val_loss: 51669.6641\n",
      "Epoch 5/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 42928.7852 - val_loss: 26729.2480\n",
      "Epoch 6/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 25705.7441 - val_loss: 23238.1875\n",
      "Epoch 7/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 23178.0000 - val_loss: 22975.0977\n",
      "Epoch 8/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23358.4258 - val_loss: 22969.9082\n",
      "Epoch 9/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23140.7090 - val_loss: 22971.9668\n",
      "Epoch 10/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23017.1387 - val_loss: 22970.4434\n",
      "Epoch 11/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23447.3965 - val_loss: 22969.2930\n",
      "Epoch 12/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23233.2480 - val_loss: 22969.6523\n",
      "Epoch 13/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23633.1152 - val_loss: 22969.8770\n",
      "Epoch 14/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23340.6113 - val_loss: 22970.0684\n",
      "Epoch 15/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23598.9316 - val_loss: 22985.8379\n",
      "Epoch 16/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23505.7207 - val_loss: 22969.9453\n",
      "Epoch 17/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23361.3965 - val_loss: 22976.0234\n",
      "Epoch 18/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23128.7656 - val_loss: 22969.2383\n",
      "Epoch 19/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23361.4805 - val_loss: 22980.3477\n",
      "Epoch 20/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23375.0566 - val_loss: 22977.4805\n",
      "Epoch 21/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23139.4355 - val_loss: 22970.4336\n",
      "Epoch 22/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23345.3125 - val_loss: 23019.4492\n",
      "Epoch 23/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23474.5781 - val_loss: 22970.9141\n",
      "Epoch 24/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23514.2363 - val_loss: 22977.6133\n",
      "Epoch 25/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23165.6074 - val_loss: 22983.4707\n",
      "Epoch 26/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23305.5410 - val_loss: 23000.3438\n",
      "Epoch 27/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23332.6582 - val_loss: 22971.0156\n",
      "Epoch 28/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 23620.4375 - val_loss: 22969.2930\n",
      "Epoch 29/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23194.9004 - val_loss: 22970.0645\n",
      "Epoch 30/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23076.8789 - val_loss: 22970.4082\n",
      "Epoch 31/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23460.8164 - val_loss: 22972.0879\n",
      "Epoch 32/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 23381.0020 - val_loss: 22974.3066\n",
      "Epoch 33/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23224.7695 - val_loss: 22994.6426\n",
      "Epoch 34/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23438.0684 - val_loss: 22989.7109\n",
      "Epoch 35/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23529.7676 - val_loss: 22973.8926\n",
      "Epoch 36/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23488.9941 - val_loss: 22970.0020\n",
      "Epoch 37/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 23046.5195 - val_loss: 22969.2598\n",
      "Epoch 38/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23651.6895 - val_loss: 22999.2852\n",
      "Epoch 39/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 23192.3379 - val_loss: 22980.1699\n",
      "Epoch 40/40\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 23701.7676 - val_loss: 22981.9043\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "-0.000553224449984846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load data\n",
    "#   df = numerical_indicies(data)\n",
    "df = ordinal_ranks(data, drop_columns=True)\n",
    "\n",
    "# Split the df into training and testing sets\n",
    "X = df.drop(columns='roster_points')\n",
    "y = df['roster_points']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = Embedding(input_dim=5, output_dim=3)(inputs)\n",
    "x = LSTM(100)(x)\n",
    "x = Dense(1)(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(optimizer=Adam(0.01), loss='mse')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x=X_train, y=y_train.values, \n",
    "          validation_data=(X_test, y_test.values), \n",
    "          batch_size=50, epochs=40)\n",
    "\n",
    "# Predict roster_points\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004462948330022032\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load data\n",
    "#   df = numerical_indicies(data)\n",
    "df = ordinal_ranks(data, drop_columns=True)\n",
    "\n",
    "# Split the df into training and testing sets\n",
    "X = df.drop(columns='roster_points')\n",
    "y = df['roster_points']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Fit an XGBoost regressor\n",
    "xgbr = xgb.XGBRegressor(n_estimators=100, learning_rate=0.001)\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict roster_points\n",
    "y_pred = xgbr.predict(X_test)\n",
    "\n",
    "# Calculate the R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding teams with the highest expect points/playoff chances; run a cluster analysis to discover the possible drafting patterns that are most successful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
